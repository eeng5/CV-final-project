# CV-final-project
### USE OUR VIRTUAL ENVIRONMENT ##
### To activate the virtual environment ###
### To Run Our Live Emotion Detection App ####
```python run.py --live```

### To Train the Simple Model: ###
```python run.py --task 1 --aug 1```  -> less filtered images in augmentation step 
```python run.py --task 1 --aug 3```  -> more filters in augmentation step 

### To Train the Complex Model: ###
```python run.py --task 3 --aug 3```

### Other Run Commands: ###
Evaluate our Simple Model:

```python run.py --task 1 --load-checkpoint checkpoints/simple_model/<TIMESTAMP>/<MODEL_FILENAME>.h5 --evaluate```

Evaluate our Complex Model:

```python run.py --task 3 --load-checkpoint checkpoints/complex_model/<TIMESTAMP>/<MODEL_FILENAME>.h5 --evaluate```

View Misclassified image from our Simple Model:

```python3 run.py --task 1 --load-checkpoint checkpoints/simple_model/<TIMESTAMP>/<MODEL_FILENAME>.h5 --evaluate --lime-image test/angry/image_0004.jpg```

View Misclassified image from our Complex Model:

```python3 run.py --task 3 --load-checkpoint checkpoints/complex_model/<TIMESTAMP>/<MODEL_FILENAME>.h5 --evaluate --lime-image test/angry/image_0004.jpg```

View Epochs during Training:

```tensorboard --logdir logs```

# To Do #
- [x] Get Data in folder 
- [x] preprocess data and augment data
- [x] create CNN pipeline for predicting emotion 
- [x] Progress Report
- [x] Get tensorboard logs 
- [x] Connect pretrained model to live video 
- [x] Presentation
- [x] Project Report 
